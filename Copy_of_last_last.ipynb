{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frederick-Teye/ML-models/blob/main/Copy_of_last_last.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk8x0g3YEKBR",
        "outputId": "92778c09-4056-4f8a-db85-807bd803af8c"
      },
      "id": "Yk8x0g3YEKBR",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c17264ce-78ac-4910-a5bb-52f2abb4b47e",
      "metadata": {
        "id": "c17264ce-78ac-4910-a5bb-52f2abb4b47e",
        "outputId": "0a1310c4-105f-4db0-9918-59748e22594a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
              "0  Female  56.0             0              0         No Info  37.23   \n",
              "1    Male  29.0             0              0           never  27.32   \n",
              "2  Female  26.0             0              0         No Info  27.32   \n",
              "3    Male  50.0             0              0         current  27.32   \n",
              "4  Female  56.0             0              0           never  38.48   \n",
              "\n",
              "   hbA1c_level  blood_glucose_level  diabetes  \n",
              "0          6.5                   80         0  \n",
              "1          3.5                  160         0  \n",
              "2          4.0                  145         0  \n",
              "3          5.0                  145         0  \n",
              "4          5.7                  155         1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-98792b6a-f028-4ecd-8660-e837c5a5ad97\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>smoking_history</th>\n",
              "      <th>bmi</th>\n",
              "      <th>hbA1c_level</th>\n",
              "      <th>blood_glucose_level</th>\n",
              "      <th>diabetes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Female</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No Info</td>\n",
              "      <td>37.23</td>\n",
              "      <td>6.5</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>never</td>\n",
              "      <td>27.32</td>\n",
              "      <td>3.5</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No Info</td>\n",
              "      <td>27.32</td>\n",
              "      <td>4.0</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>current</td>\n",
              "      <td>27.32</td>\n",
              "      <td>5.0</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Female</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>never</td>\n",
              "      <td>38.48</td>\n",
              "      <td>5.7</td>\n",
              "      <td>155</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98792b6a-f028-4ecd-8660-e837c5a5ad97')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-98792b6a-f028-4ecd-8660-e837c5a5ad97 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-98792b6a-f028-4ecd-8660-e837c5a5ad97');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ziya_data",
              "summary": "{\n  \"name\": \"ziya_data\",\n  \"rows\": 17000,\n  \"fields\": [\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Male\",\n          \"Female\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21.483865092644045,\n        \"min\": 0.08,\n        \"max\": 80.0,\n        \"num_unique_values\": 102,\n        \"samples\": [\n          51.0,\n          63.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hypertension\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"heart_disease\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smoking_history\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"No Info\",\n          \"never\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bmi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.48984736386505,\n        \"min\": 10.77,\n        \"max\": 88.72,\n        \"num_unique_values\": 3324,\n        \"samples\": [\n          17.35,\n          24.08\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hbA1c_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2773176429765467,\n        \"min\": 3.5,\n        \"max\": 9.0,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          6.5,\n          3.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"blood_glucose_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56,\n        \"min\": 80,\n        \"max\": 300,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          80,\n          160\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"diabetes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import pandas as pd\n",
        "ziya_data = pd.read_csv('/content/drive/MyDrive/data-for-datamining-class/kaggle/cleaned_ziya_data.csv')\n",
        "ziya_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d266cba",
        "outputId": "7022f2ee-6644-474d-d189-cd9e34b7db2b"
      },
      "source": [
        "import os\n",
        "print(os.listdir('.'))"
      ],
      "id": "6d266cba",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'drive', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f70e613"
      },
      "source": [
        "Please upload the `cleaned_ziya_data.csv` file into your Colab environment. You can do this by clicking the folder icon on the left sidebar, then clicking the 'Upload to session storage' icon, and selecting your file. After uploading, run the next cell to load the data."
      ],
      "id": "6f70e613"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "46f6e4d8",
        "outputId": "261b462c-73d3-4f99-f866-d999dd80f853"
      },
      "source": [
        "ziya_data.head()"
      ],
      "id": "46f6e4d8",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
              "0  Female  56.0             0              0         No Info  37.23   \n",
              "1    Male  29.0             0              0           never  27.32   \n",
              "2  Female  26.0             0              0         No Info  27.32   \n",
              "3    Male  50.0             0              0         current  27.32   \n",
              "4  Female  56.0             0              0           never  38.48   \n",
              "\n",
              "   hbA1c_level  blood_glucose_level  diabetes  \n",
              "0          6.5                   80         0  \n",
              "1          3.5                  160         0  \n",
              "2          4.0                  145         0  \n",
              "3          5.0                  145         0  \n",
              "4          5.7                  155         1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86a96aba-1370-4a7b-9d43-026949ea6776\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>smoking_history</th>\n",
              "      <th>bmi</th>\n",
              "      <th>hbA1c_level</th>\n",
              "      <th>blood_glucose_level</th>\n",
              "      <th>diabetes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Female</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No Info</td>\n",
              "      <td>37.23</td>\n",
              "      <td>6.5</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>never</td>\n",
              "      <td>27.32</td>\n",
              "      <td>3.5</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No Info</td>\n",
              "      <td>27.32</td>\n",
              "      <td>4.0</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>current</td>\n",
              "      <td>27.32</td>\n",
              "      <td>5.0</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Female</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>never</td>\n",
              "      <td>38.48</td>\n",
              "      <td>5.7</td>\n",
              "      <td>155</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86a96aba-1370-4a7b-9d43-026949ea6776')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-86a96aba-1370-4a7b-9d43-026949ea6776 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-86a96aba-1370-4a7b-9d43-026949ea6776');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ziya_data",
              "summary": "{\n  \"name\": \"ziya_data\",\n  \"rows\": 17000,\n  \"fields\": [\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Male\",\n          \"Female\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21.483865092644045,\n        \"min\": 0.08,\n        \"max\": 80.0,\n        \"num_unique_values\": 102,\n        \"samples\": [\n          51.0,\n          63.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hypertension\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"heart_disease\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smoking_history\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"No Info\",\n          \"never\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bmi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.48984736386505,\n        \"min\": 10.77,\n        \"max\": 88.72,\n        \"num_unique_values\": 3324,\n        \"samples\": [\n          17.35,\n          24.08\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hbA1c_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2773176429765467,\n        \"min\": 3.5,\n        \"max\": 9.0,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          6.5,\n          3.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"blood_glucose_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56,\n        \"min\": 80,\n        \"max\": 300,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          80,\n          160\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"diabetes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "692fd18c-360f-434c-ac27-c4c5baadaf99",
      "metadata": {
        "id": "692fd18c-360f-434c-ac27-c4c5baadaf99",
        "outputId": "26476b38-0eb1-4a5f-ceb0-689d6e1cb570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gender  hypertension  heart_disease\n",
              "Female  0             0                7620\n",
              "                      1                 439\n",
              "        1             0                1207\n",
              "                      1                 184\n",
              "Male    0             0                5674\n",
              "                      1                 673\n",
              "        1             0                 979\n",
              "                      1                 224\n",
              "Name: diabetes, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>diabetes</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Female</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
              "      <th>0</th>\n",
              "      <td>7620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
              "      <th>0</th>\n",
              "      <td>1207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Male</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
              "      <th>0</th>\n",
              "      <td>5674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
              "      <th>0</th>\n",
              "      <td>979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>224</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "ziya_data.groupby(['gender', 'hypertension', 'heart_disease']).diabetes.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "51a966b3-ee91-4781-a5a4-5af14a5ca533",
      "metadata": {
        "id": "51a966b3-ee91-4781-a5a4-5af14a5ca533",
        "outputId": "4a8f42f4-d5fc-4e57-82f1-b9c906405298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "diabetes\n",
              "0    8500\n",
              "1    8500\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diabetes</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "ziya_data.diabetes.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2393b7c8-491e-41db-a26a-09ce6b075b95",
      "metadata": {
        "id": "2393b7c8-491e-41db-a26a-09ce6b075b95"
      },
      "outputs": [],
      "source": [
        "# Balanced code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "8cf34c66-2960-459e-982d-ef35afef3150",
      "metadata": {
        "id": "8cf34c66-2960-459e-982d-ef35afef3150",
        "outputId": "5584c336-ce64-4fff-996a-21293930148d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation at Threshold = 0.50\n",
            "==================================================\n",
            "Accuracy: 0.9091\n",
            "Precision: 0.9009\n",
            "Recall: 0.9194\n",
            "F1-Score: 0.9100\n",
            "ROC-AUC: 0.9766\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91      1700\n",
            "           1       0.90      0.92      0.91      1700\n",
            "\n",
            "    accuracy                           0.91      3400\n",
            "   macro avg       0.91      0.91      0.91      3400\n",
            "weighted avg       0.91      0.91      0.91      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, classification_report)\n",
        "\n",
        "# Load data\n",
        "X = ziya_data.drop(columns=['diabetes'])  # Feature matrix\n",
        "y = ziya_data['diabetes']  # Target variable\n",
        "\n",
        "# Train-test split (stratify to maintain balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature selection\n",
        "num_features = ['age', 'hypertension', 'heart_disease', 'bmi',\n",
        "                'hbA1c_level', 'blood_glucose_level']\n",
        "cat_features = ['gender', 'smoking_history']\n",
        "\n",
        "# Categorical Encoding (One-Hot Encoding)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# XGBoost Classifier (No class weighting for balanced data)\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=900,            # Number of boosting rounds\n",
        "    learning_rate=0.01,          # Learning rate (lower = better generalization)\n",
        "    early_stopping_rounds=10,    # Stop training if validation loss doesn't improve\n",
        "    eval_metric=['aucpr', 'logloss'],  # Evaluation metrics\n",
        "    random_state=42,             # Random seed for reproducibility\n",
        "    max_depth=6,                 # Tree depth (higher = more complex model)\n",
        "    min_child_weight=3,          # Regularization parameter\n",
        "    subsample=0.8,               # Fraction of samples used per boosting round\n",
        "    colsample_bytree=0.9         # Fraction of features used per tree\n",
        ")\n",
        "\n",
        "# Training with validation set\n",
        "eval_set = [(X_train_preprocessed, y_train), (X_test_preprocessed, y_test)]\n",
        "xgb_model.fit(X_train_preprocessed, y_train, eval_set=eval_set, verbose=False)\n",
        "\n",
        "# Get probabilities\n",
        "y_pred_proba = xgb_model.predict_proba(X_test_preprocessed)[:, 1]\n",
        "\n",
        "# Decision Threshold\n",
        "threshold = 0.5  # Standard for balanced data\n",
        "y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "# Model Evaluation\n",
        "print(f\"\\nEvaluation at Threshold = {threshold:.2f}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Store metrics for visualization\n",
        "recall_model0 = recall_score(y_test, y_pred)\n",
        "precision_model0 = precision_score(y_test, y_pred)\n",
        "f1_model0 = f1_score(y_test, y_pred)\n",
        "accuracy_model0 = accuracy_score(y_test, y_pred)\n",
        "roc_auc_model0 = roc_auc_score(y_test, y_pred_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9c47a17f-4002-4130-ac64-f04b5171dede",
      "metadata": {
        "id": "9c47a17f-4002-4130-ac64-f04b5171dede",
        "outputId": "036add67-9ee8-4348-9c73-a684c937bd99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation at Threshold = 0.50\n",
            "==================================================\n",
            "Accuracy: 0.9091\n",
            "Precision: 0.9009\n",
            "Recall: 0.9194\n",
            "F1-Score: 0.9100\n",
            "ROC-AUC: 0.9766\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91      1700\n",
            "           1       0.90      0.92      0.91      1700\n",
            "\n",
            "    accuracy                           0.91      3400\n",
            "   macro avg       0.91      0.91      0.91      3400\n",
            "weighted avg       0.91      0.91      0.91      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, classification_report)\n",
        "\n",
        "# Load data\n",
        "X = ziya_data.drop(columns=['diabetes'])  # Feature matrix\n",
        "y = ziya_data['diabetes']  # Target variable\n",
        "\n",
        "# Train-test split (stratify to maintain balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature selection\n",
        "num_features = ['age', 'hypertension', 'heart_disease', 'bmi',\n",
        "                'hbA1c_level', 'blood_glucose_level']\n",
        "cat_features = ['gender', 'smoking_history']\n",
        "\n",
        "# Categorical Encoding (One-Hot Encoding)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# XGBoost Classifier (No class weighting for balanced data)\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=1000,            # Number of boosting rounds\n",
        "    learning_rate=0.01,          # Learning rate (lower = better generalization)\n",
        "    early_stopping_rounds=10,    # Stop training if validation loss doesn't improve\n",
        "    eval_metric=['aucpr', 'logloss'],  # Evaluation metrics\n",
        "    random_state=42,             # Random seed for reproducibility\n",
        "    max_depth=6,                 # Tree depth (higher = more complex model)\n",
        "    min_child_weight=3,          # Regularization parameter\n",
        "    subsample=0.8,               # Fraction of samples used per boosting round\n",
        "    colsample_bytree=0.9         # Fraction of features used per tree\n",
        ")\n",
        "\n",
        "# Training with validation set\n",
        "eval_set = [(X_train_preprocessed, y_train), (X_test_preprocessed, y_test)]\n",
        "xgb_model.fit(X_train_preprocessed, y_train, eval_set=eval_set, verbose=False)\n",
        "\n",
        "# Get probabilities\n",
        "y_pred_proba = xgb_model.predict_proba(X_test_preprocessed)[:, 1]\n",
        "\n",
        "# Decision Threshold\n",
        "threshold = 0.5  # Standard for balanced data\n",
        "y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "# Model Evaluation\n",
        "print(f\"\\nEvaluation at Threshold = {threshold:.2f}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Store metrics for visualization\n",
        "recall_model0 = recall_score(y_test, y_pred)\n",
        "precision_model0 = precision_score(y_test, y_pred)\n",
        "f1_model0 = f1_score(y_test, y_pred)\n",
        "accuracy_model0 = accuracy_score(y_test, y_pred)\n",
        "roc_auc_model0 = roc_auc_score(y_test, y_pred_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "086aa2fd-9a7d-4ace-baa4-549cafb54e30",
      "metadata": {
        "id": "086aa2fd-9a7d-4ace-baa4-549cafb54e30"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "61c3ebff-cc51-4e8f-b018-176291be9a5d",
      "metadata": {
        "id": "61c3ebff-cc51-4e8f-b018-176291be9a5d"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning using Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c2aef54b-2313-4893-8702-16d91f2edbd3",
      "metadata": {
        "id": "c2aef54b-2313-4893-8702-16d91f2edbd3",
        "outputId": "5967d341-a15b-479b-9626-bc007aae2b69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version: 1.6.1\n",
            "xgboost version: 3.1.3\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "import xgboost\n",
        "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
        "print(f\"xgboost version: {xgboost.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gab9mgSXDgO_"
      },
      "id": "gab9mgSXDgO_",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "2d84c94c-d470-400e-b53f-ca3fb0593070",
      "metadata": {
        "id": "2d84c94c-d470-400e-b53f-ca3fb0593070"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "65983605-336c-483a-80a9-747b66759ee0",
      "metadata": {
        "id": "65983605-336c-483a-80a9-747b66759ee0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c57cbfb1-52a8-41a3-9bcc-a82ee9a81939",
      "metadata": {
        "id": "c57cbfb1-52a8-41a3-9bcc-a82ee9a81939"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3b841bde-a513-4b3e-bcab-ed5fa3add2f0",
      "metadata": {
        "id": "3b841bde-a513-4b3e-bcab-ed5fa3add2f0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f8912fc3-3b44-459b-ab5f-aa89c35d7e07",
      "metadata": {
        "id": "f8912fc3-3b44-459b-ab5f-aa89c35d7e07"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b5d3df9a-f5d6-4b8f-a1ae-87e3db124ca3",
      "metadata": {
        "id": "b5d3df9a-f5d6-4b8f-a1ae-87e3db124ca3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "30b22a00-20f8-4b0d-bc7b-28c2a34aeb5a",
      "metadata": {
        "id": "30b22a00-20f8-4b0d-bc7b-28c2a34aeb5a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5537458b-4b47-4ae0-b6ab-7e40872f00de",
      "metadata": {
        "id": "5537458b-4b47-4ae0-b6ab-7e40872f00de"
      },
      "source": [
        "# hpelm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9de71911-b82f-46b9-8f05-1a2318c55e26",
      "metadata": {
        "id": "9de71911-b82f-46b9-8f05-1a2318c55e26",
        "outputId": "97849d73-010e-402e-f61c-8faa38265700",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hpelm in /usr/local/lib/python3.12/dist-packages (1.0.10)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.12/dist-packages (from hpelm) (0.20)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.12/dist-packages (from hpelm) (1.3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from hpelm) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.12 in /usr/local/lib/python3.12/dist-packages (from hpelm) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from hpelm) (1.17.0)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.12/dist-packages (from hpelm) (3.10.2)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.12/dist-packages (from tables->hpelm) (2.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tables->hpelm) (26.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from tables->hpelm) (9.0.0)\n",
            "Requirement already satisfied: blosc2>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from tables->hpelm) (4.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from tables->hpelm) (4.15.0)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.12/dist-packages (from blosc2>=2.3.0->tables->hpelm) (1.10.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from blosc2>=2.3.0->tables->hpelm) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from blosc2>=2.3.0->tables->hpelm) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->blosc2>=2.3.0->tables->hpelm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->blosc2>=2.3.0->tables->hpelm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->blosc2>=2.3.0->tables->hpelm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->blosc2>=2.3.0->tables->hpelm) (2026.1.4)\n",
            " X_train_preprocessed.shape[1]:\n",
            "  12\n",
            "y_train.shape[1]:\n",
            " 2\n",
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8682\n",
            "Precision: 0.8687\n",
            "Recall: 0.8676\n",
            "F1-Score: 0.8682\n",
            "ROC-AUC: 0.9387\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87      1700\n",
            "           1       0.87      0.87      0.87      1700\n",
            "\n",
            "    accuracy                           0.87      3400\n",
            "   macro avg       0.87      0.87      0.87      3400\n",
            "weighted avg       0.87      0.87      0.87      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install hpelm\n",
        "import hpelm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "\n",
        "# Load data\n",
        "X = ziya_data.drop(columns=['diabetes'])\n",
        "y = ziya_data['diabetes']\n",
        "\n",
        "# Convert target variable to one-hot encoding\n",
        "ohe = OneHotEncoder(drop=None, sparse_output=False)\n",
        "y_onehot = ohe.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Split data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define available features\n",
        "available_features = X_train.columns.tolist()\n",
        "\n",
        "# Preprocessing pipeline\n",
        "num_features = ['age', 'hypertension', 'heart_disease', 'bmi', 'hbA1c_level', 'blood_glucose_level']\n",
        "cat_features = ['gender', 'smoking_history']\n",
        "\n",
        "# Remove missing features\n",
        "num_features = [col for col in num_features if col in available_features]\n",
        "cat_features = [col for col in cat_features if col in available_features]\n",
        "\n",
        "# Define preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply transformations\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "print(f' X_train_preprocessed.shape[1]:\\n  {X_train_preprocessed.shape[1]}')\n",
        "print(f'y_train.shape[1]:\\n {y_train.shape[1]}')\n",
        "\n",
        "# Define the ELM model\n",
        "num_neurons = 3000  # Adjust as needed\n",
        "elm_model = hpelm.ELM(X_train_preprocessed.shape[1], y_train.shape[1])\n",
        "elm_model.add_neurons(num_neurons, \"sigm\")  # Using sigmoid activation\n",
        "\n",
        "# Train the model\n",
        "elm_model.train(X_train_preprocessed, y_train, \"c\")  # Classification mode\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = elm_model.predict(X_test_preprocessed)\n",
        "\n",
        "# Apply threshold for classification\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "# Convert y_test back to single-column labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test_labels, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_labels, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b12fb87b-98a2-40bd-b3d0-a28e3f4e0cf1",
      "metadata": {
        "id": "b12fb87b-98a2-40bd-b3d0-a28e3f4e0cf1",
        "outputId": "c7ff8efd-07cc-4eb2-c4c3-b39b5f08e3fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8729\n",
            "Precision: 0.8760\n",
            "Recall: 0.8688\n",
            "F1-Score: 0.8724\n",
            "ROC-AUC: 0.9422\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87      1700\n",
            "           1       0.88      0.87      0.87      1700\n",
            "\n",
            "    accuracy                           0.87      3400\n",
            "   macro avg       0.87      0.87      0.87      3400\n",
            "weighted avg       0.87      0.87      0.87      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import hpelm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "\n",
        "# Load data\n",
        "X = ziya_data.drop(columns=['diabetes'])\n",
        "y = ziya_data['diabetes']\n",
        "\n",
        "# Convert target variable to one-hot encoding\n",
        "ohe = OneHotEncoder(drop=None, sparse_output=False)\n",
        "y_onehot = ohe.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Split data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define available features\n",
        "available_features = X_train.columns.tolist()\n",
        "\n",
        "# Preprocessing pipeline\n",
        "num_features = ['age', 'hypertension', 'heart_disease', 'bmi', 'hbA1c_level', 'blood_glucose_level']\n",
        "cat_features = ['gender', 'smoking_history']\n",
        "\n",
        "# Remove missing features\n",
        "num_features = [col for col in num_features if col in available_features]\n",
        "cat_features = [col for col in cat_features if col in available_features]\n",
        "\n",
        "# Define preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply transformations\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Define the ELM model\n",
        "num_neurons = 3000  # Adjust as needed\n",
        "elm_model = hpelm.ELM(X_train_preprocessed.shape[1], y_train.shape[1])\n",
        "elm_model.add_neurons(num_neurons, \"sigm\")  # Using sigmoid activation\n",
        "\n",
        "# Train the model\n",
        "elm_model.train(X_train_preprocessed, y_train, \"c\")  # Classification mode\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = elm_model.predict(X_test_preprocessed)\n",
        "\n",
        "# Apply threshold for classification\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "# Convert y_test back to single-column labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test_labels, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_labels, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "6c7ac0ab-cb56-4b79-97f2-b25f9c00ce16",
      "metadata": {
        "id": "6c7ac0ab-cb56-4b79-97f2-b25f9c00ce16"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "46c6439c-d175-4fc0-bb9d-c1cee7a3d68f",
      "metadata": {
        "id": "46c6439c-d175-4fc0-bb9d-c1cee7a3d68f",
        "outputId": "f1246246-b34c-4c89-c9b7-5a2b3801f9e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8303\n",
            "Precision: 0.7985\n",
            "Recall: 0.8835\n",
            "F1-Score: 0.8389\n",
            "ROC-AUC: 0.8860\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.78      0.82      1700\n",
            "           1       0.80      0.88      0.84      1700\n",
            "\n",
            "    accuracy                           0.83      3400\n",
            "   macro avg       0.83      0.83      0.83      3400\n",
            "weighted avg       0.83      0.83      0.83      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import hpelm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, classification_report, precision_recall_curve)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load data (same as HPELM)\n",
        "X = ziya_data.drop(columns=['diabetes'])\n",
        "y = ziya_data['diabetes']\n",
        "\n",
        "# Feature Engineering: Create BMI categories\n",
        "X['bmi_category'] = pd.cut(X['bmi'], bins=[0, 18.5, 25, 30, np.inf], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Convert target variable to one-hot encoding\n",
        "ohe = OneHotEncoder(drop=None, sparse_output=False)\n",
        "y_onehot = ohe.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Split data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define available features\n",
        "available_features = X_train.columns.tolist()\n",
        "\n",
        "# Preprocessing pipeline\n",
        "num_features = ['age', 'hypertension', 'heart_disease',\n",
        "       'bmi', 'hbA1c_level', 'blood_glucose_level']\n",
        "cat_features = ['gender', 'smoking_history', 'bmi_category']  # Added bmi_category\n",
        "\n",
        "# Remove missing features\n",
        "num_features = [col for col in num_features if col in available_features]\n",
        "cat_features = [col for col in cat_features if col in available_features]\n",
        "\n",
        "# Define preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', RobustScaler(), num_features),  # Changed to RobustScaler\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)  # Removed drop='first'\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply transformations\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Dynamically adjust the number of neurons\n",
        "num_neurons = min(5000, X_train_preprocessed.shape[0] // 2)\n",
        "\n",
        "# Define the ELM model\n",
        "elm_model = hpelm.ELM(X_train_preprocessed.shape[1], y_train.shape[1])\n",
        "elm_model.add_neurons(num_neurons, \"sigm\")  # Using sigmoid activation\n",
        "\n",
        "# Train the model with class weights\n",
        "elm_model.train(X_train_preprocessed, y_train, \"c\", W=class_weight_dict)  # Classification mode\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = elm_model.predict(X_test_preprocessed)\n",
        "\n",
        "# Determine best threshold using Precision-Recall Curve\n",
        "precisions, recalls, thresholds = precision_recall_curve(np.argmax(y_test, axis=1), y_pred_proba[:, 1])\n",
        "best_threshold = thresholds[np.argmax(precisions + recalls)]\n",
        "\n",
        "# Apply optimized threshold\n",
        "y_pred = (y_pred_proba[:, 1] >= best_threshold).astype(int)\n",
        "\n",
        "# Convert y_test back to single-column labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test_labels, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_labels, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3b1e7dfd-9dde-4ff3-b682-1405eddaa7ca",
      "metadata": {
        "id": "3b1e7dfd-9dde-4ff3-b682-1405eddaa7ca",
        "outputId": "7c4784c6-f6de-4451-fedb-e70c83f14160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8382\n",
            "Precision: 0.8142\n",
            "Recall: 0.8765\n",
            "F1-Score: 0.8442\n",
            "ROC-AUC: 0.8953\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.83      1700\n",
            "           1       0.81      0.88      0.84      1700\n",
            "\n",
            "    accuracy                           0.84      3400\n",
            "   macro avg       0.84      0.84      0.84      3400\n",
            "weighted avg       0.84      0.84      0.84      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import hpelm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, classification_report, precision_recall_curve)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load data (same as HPELM)\n",
        "X = ziya_data.drop(columns=['diabetes'])\n",
        "y = ziya_data['diabetes']\n",
        "\n",
        "# Feature Engineering: Create BMI categories\n",
        "X['bmi_category'] = pd.cut(X['bmi'], bins=[0, 18.5, 25, 30, np.inf], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Convert target variable to one-hot encoding\n",
        "ohe = OneHotEncoder(drop=None, sparse_output=False)\n",
        "y_onehot = ohe.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Split data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define available features\n",
        "available_features = X_train.columns.tolist()\n",
        "\n",
        "# Preprocessing pipeline\n",
        "num_features = ['age', 'hypertension', 'heart_disease',\n",
        "       'bmi', 'hbA1c_level', 'blood_glucose_level']\n",
        "cat_features = ['gender', 'smoking_history', 'bmi_category']  # Added bmi_category\n",
        "\n",
        "# Remove missing features\n",
        "num_features = [col for col in num_features if col in available_features]\n",
        "cat_features = [col for col in cat_features if col in available_features]\n",
        "\n",
        "# Define preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', RobustScaler(), num_features),  # Changed to RobustScaler\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)  # Removed drop='first'\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply transformations\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Dynamically adjust the number of neurons\n",
        "num_neurons = min(5000, X_train_preprocessed.shape[0] // 2)\n",
        "\n",
        "# Define the ELM model\n",
        "elm_model = hpelm.ELM(X_train_preprocessed.shape[1], y_train.shape[1])\n",
        "elm_model.add_neurons(num_neurons, \"sigm\")  # Using sigmoid activation\n",
        "\n",
        "# Train the model with class weights\n",
        "elm_model.train(X_train_preprocessed, y_train, \"c\", W=class_weight_dict)  # Classification mode\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = elm_model.predict(X_test_preprocessed)\n",
        "\n",
        "# Determine best threshold using Precision-Recall Curve\n",
        "precisions, recalls, thresholds = precision_recall_curve(np.argmax(y_test, axis=1), y_pred_proba[:, 1])\n",
        "best_threshold = thresholds[np.argmax(precisions + recalls)]\n",
        "\n",
        "# Apply optimized threshold\n",
        "y_pred = (y_pred_proba[:, 1] >= best_threshold).astype(int)\n",
        "\n",
        "# Convert y_test back to single-column labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test_labels, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test_labels, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_labels, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "642e4170-59f9-4d9d-ac33-ea2f4a390f8c",
      "metadata": {
        "id": "642e4170-59f9-4d9d-ac33-ea2f4a390f8c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "0d0bafaf-6026-4e88-98b0-26db7580f8a7",
      "metadata": {
        "id": "0d0bafaf-6026-4e88-98b0-26db7580f8a7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0cb03c02-ceaa-4495-9297-2f5977a084f7",
      "metadata": {
        "id": "0cb03c02-ceaa-4495-9297-2f5977a084f7",
        "outputId": "e403fa59-a46d-4c25-c4f3-600bad9c7140",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hpelm/nnets/slfn.py:62: RuntimeWarning: overflow encountered in exp\n",
            "  self.func[\"sigm\"] = lambda X, W, B: 1 / (1 + np.exp(np.dot(X, W) + B))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing batch 12/14, eta 0:00:00\n",
            "Covariance matrix is not full rank; solving with SVD (slow)\n",
            "This happened because you have duplicated or too many neurons\n",
            "Accuracy: 0.8582352941176471\n"
          ]
        }
      ],
      "source": [
        "from hpelm import HPELM\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X = ziya_data.drop(columns=['diabetes'])\n",
        "y = ziya_data['diabetes']\n",
        "\n",
        "# Split data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Preprocessing pipeline\n",
        "num_features = ['age', 'hypertension', 'heart_disease', 'bmi',\n",
        "                'hbA1c_level', 'blood_glucose_level']\n",
        "cat_features = ['gender', 'smoking_history']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# One-hot encode the target variable for HPELM\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1)) # For consistency if needed later\n",
        "\n",
        "# Initialize and train HPELM model\n",
        "inputs = X_train_processed.shape[1]  # Number of features\n",
        "outputs = y_train_encoded.shape[1]   # Number of classes from one-hot encoding\n",
        "model = HPELM(inputs, outputs, classification='c')  # Use 'c' for classification\n",
        "\n",
        "print(f'X: {len(X.columns)}')\n",
        "# Removed print statements for X_train_processed as they were verbose\n",
        "\n",
        "# Add neurons to the model (example value, adjust as needed)\n",
        "model.add_neurons(3000, 'sigm')\n",
        "\n",
        "# Train model with one-hot encoded target\n",
        "model.train(X_train_processed, y_train_encoded)\n",
        "\n",
        "# Predict using the model (predicts one-hot encoded output)\n",
        "y_pred_encoded = model.predict(X_test_processed)\n",
        "\n",
        "# Decode predictions back to original labels (0 or 1)\n",
        "y_pred = encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "87184bbe-ca4d-41a7-8c85-280e3e79e3b6",
      "metadata": {
        "id": "87184bbe-ca4d-41a7-8c85-280e3e79e3b6",
        "outputId": "c2f5fedc-3d85-4789-864d-c43e3f72773a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hpelm/nnets/slfn.py:62: RuntimeWarning: overflow encountered in exp\n",
            "  self.func[\"sigm\"] = lambda X, W, B: 1 / (1 + np.exp(np.dot(X, W) + B))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing batch 11/14, eta 0:00:01\n",
            "Covariance matrix is not full rank; solving with SVD (slow)\n",
            "This happened because you have duplicated or too many neurons\n",
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8371\n",
            "Precision: 0.8111\n",
            "Recall: 0.8788\n",
            "F1-Score: 0.8436\n",
            "ROC-AUC: 0.8953\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.83      1700\n",
            "           1       0.81      0.88      0.84      1700\n",
            "\n",
            "    accuracy                           0.84      3400\n",
            "   macro avg       0.84      0.84      0.84      3400\n",
            "weighted avg       0.84      0.84      0.84      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from hpelm import HPELM\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, classification_report, precision_recall_curve)\n",
        "\n",
        "# Load data\n",
        "X = ziya_data.drop(columns=['diabetes'])\n",
        "y = ziya_data['diabetes']\n",
        "\n",
        "# Split data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define numerical and categorical features\n",
        "num_features = ['age', 'hypertension', 'heart_disease', 'bmi',\n",
        "                'hbA1c_level', 'blood_glucose_level']\n",
        "cat_features = ['gender', 'smoking_history']\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))  # For predictions\n",
        "\n",
        "# Initialize HPELM model\n",
        "inputs = X_train_processed.shape[1]  # Number of features\n",
        "outputs = y_train_encoded.shape[1]  # Number of classes\n",
        "model = HPELM(inputs, outputs, classification='c')  # Use 'c' for classification\n",
        "\n",
        "# Add neurons to the model\n",
        "model.add_neurons(3000, 'sigm')  # Add 100 sigmoid neurons, for example\n",
        "\n",
        "# Train model\n",
        "model.train(X_train_processed, y_train_encoded)\n",
        "\n",
        "# Predict using the model\n",
        "y_pred_encoded = model.predict(X_test_processed)\n",
        "\n",
        "# Decode predictions back to original labels\n",
        "y_pred = encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4006ed6e-2aa3-424f-9693-c09f0c9bba1c",
      "metadata": {
        "id": "4006ed6e-2aa3-424f-9693-c09f0c9bba1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc95b68c-9570-466f-a6cb-1fee7345eb4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class HPELM in module hpelm.hp_elm:\n",
            "\n",
            "class HPELM(hpelm.elm.ELM)\n",
            " |  HPELM(inputs, outputs, classification='', w=None, batch=1000, accelerator=None, precision='double', norm=None, tprint=5)\n",
            " |\n",
            " |  Interface for training High-Performance Extreme Learning Machines (HP-ELM).\n",
            " |\n",
            " |  Args:\n",
            " |      inputs (int): dimensionality of input data, or number of data features\n",
            " |      outputs (int): dimensionality of output data, or number of classes\n",
            " |      classification ('c'/'wc'/'ml', optional): train ELM for classfication ('c') / weighted classification ('wc') /\n",
            " |          multi-label classification ('ml'). For weighted classification you can provide weights in `w`. ELM will\n",
            " |          compute and use the corresponding classification error instead of Mean Squared Error.\n",
            " |      w (vector, optional): weights vector for weighted classification, lenght (`outputs` * 1).\n",
            " |      batch (int, optional): batch size for data processing in ELM, reduces memory requirements. Does not work\n",
            " |          for model structure selection (validation, cross-validation, Leave-One-Out). Can be changed later directly\n",
            " |          as a class attribute.\n",
            " |      accelerator (string, optional): type of accelerated ELM to use: None, 'GPU', ...\n",
            " |      precision (optional): data precision to use, supports signle ('single', '32' or numpy.float32) or double\n",
            " |          ('double', '64' or numpy.float64). Single precision is faster but may cause numerical errors. Majority\n",
            " |          of GPUs work in single precision. Default: **double**.\n",
            " |      norm (double, optinal): L2-normalization parameter, **None** gives the default value.\n",
            " |      tprint (int, optional): ELM reports its progess every `tprint` seconds or after every batch,\n",
            " |          whatever takes longer.\n",
            " |\n",
            " |  Class attributes; attributes that simply store initialization or `train()` parameters are omitted.\n",
            " |\n",
            " |  Attributes:\n",
            " |      nnet (object): Implementation of neural network with computational methods, but without\n",
            " |          complex logic. Different implementations are given by different classes: for Python, for GPU, etc.\n",
            " |          See ``hpelm.nnets`` folder for particular files. You can implement your own computational algorithm\n",
            " |          by inheriting from ``hpelm.nnets.SLFN`` and overwriting some methods.\n",
            " |      flist (list of strings): Awailable types of neurons, use them when adding new neurons.\n",
            " |\n",
            " |  Note:\n",
            " |      The 'hdf5' type denotes a name of HDF5 file type with a single 2-dimensional array inside. HPELM uses PyTables\n",
            " |      interface to HDF5: http://www.pytables.org/. For HDF5 array examples, see\n",
            " |      http://www.pytables.org/usersguide/libref/homogenous_storage.html. Array name is irrelevant,\n",
            " |      but there must be **only one array per HDF5 file**.\n",
            " |\n",
            " |      A 2-dimensional Numpy.ndarray can also be used.\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      HPELM\n",
            " |      hpelm.elm.ELM\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  add_data(self, fX, fT, istart=0, icount=inf, fHH=None, fHT=None)\n",
            " |      Feed new training data (X,T) to HP-ELM model in batches: does not solve ELM itself.\n",
            " |\n",
            " |      This method prepares an intermediate solution data, that takes the most time. After that, obtaining\n",
            " |      the solution is fast.\n",
            " |\n",
            " |      The intermediate solution consists of two matrices: `HH` and `HT`. They can be in memory for a model computed\n",
            " |      at once, or stored on disk for a model computed in parts or in parallel.\n",
            " |\n",
            " |      For iterative solution, provide file names for on-disk matrices in the input parameters `fHH` and `fHT`.\n",
            " |      They will be created if they don't exist, or new results will be merged with the existing ones. This method is\n",
            " |      multiprocess-safe for parallel writing into files `fHH` and `fHT`, that allows you to easily compute ELM\n",
            " |      in parallel. The multiprocess-safeness uses Python module 'fasteners' and a lock file, which is named\n",
            " |      fHH+'.lock' and fHT+'.lock'.\n",
            " |\n",
            " |      Args:\n",
            " |          fX (hdf5): (part of) input training data size (N * `inputs`)\n",
            " |          fT (hdf5) (part of) output training data size (N * `outputs`)\n",
            " |          istart (int, optional): index of first data sample to use from `fX`, `istart` < N. If not given,\n",
            " |              all data from `fX` is used. Sample with index `istart` is used for training, indexing is 0-based.\n",
            " |          icount (int, optional): number of data samples to use from `fX`, starting from `istart`, automatically\n",
            " |              adjusted to `istart` + `icount` <= N. If not given, all data starting from `start` is used.\n",
            " |              The last sample used for training is `istart`+`icount`-1, so you can index data as:\n",
            " |              istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...\n",
            " |          fHH, fHT (string, optional): file names for storing HH and HT matrices. Files are created if they don't\n",
            " |              exist, or new result is added to the existing files if they exist. Parallel writing to the same\n",
            " |              `fHH`, `fHT` files is multiprocess-safe, made specially for parallel training of HP-ELM. Another use\n",
            " |              is to split a very long training of huge ELM into smaller parts, so the training can be interrupted\n",
            " |              and resumed later.\n",
            " |\n",
            " |  add_data_async(self, fX, fT, istart=0, icount=inf, fHH=None, fHT=None)\n",
            " |      Version of `add_data()` with asyncronous I/O. See `add_data()` for reference.\n",
            " |\n",
            " |      Spawns new processes using Python's `multiprocessing` module, and requires more memory than non-async version.\n",
            " |\n",
            " |  error(self, fT, fY, istart=0, icount=inf)\n",
            " |      Calculate error of model predictions of HPELM.\n",
            " |\n",
            " |      Computes Mean Squared Error (MSE) between model predictions Y and true outputs T.\n",
            " |      For classification, computes mis-classification error.\n",
            " |      For multi-label classification, correct classes are all with Y>0.5.\n",
            " |\n",
            " |      For weighted classification the error is an average weighted True Positive Rate,\n",
            " |      or percentage of correctly predicted samples for each class, multiplied by weight\n",
            " |      of that class and averaged. If you want something else, just write it yourself :)\n",
            " |      See https://en.wikipedia.org/wiki/Confusion_matrix for details.\n",
            " |\n",
            " |      Args:\n",
            " |          fT (hdf5): hdf5 filename with true outputs\n",
            " |          fY (hdf5): hdf5 filename with predicted outputs\n",
            " |          istart (int, optional): index of first data sample to use from `fX`, `istart` < N. If not given,\n",
            " |              all data from `fX` is used. Sample with index `istart` is used for training, indexing is 0-based.\n",
            " |          icount (int, optional): number of data samples to use from `fX`, starting from `istart`, automatically\n",
            " |              adjusted to `istart` + `icount` <= N. If not given, all data starting from `start` is used.\n",
            " |              The last sample used for training is `istart`+`icount`-1, so you can index data as:\n",
            " |              istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...\n",
            " |\n",
            " |      Returns:\n",
            " |          e (double): MSE for regression / classification error for classification.\n",
            " |\n",
            " |  predict(self, fX, fY=None, istart=0, icount=inf)\n",
            " |      Iterative predict outputs and save them to HDF5, can use custom range.\n",
            " |\n",
            " |      Args:\n",
            " |          fX (hdf5): hdf5 filename or Numpy matrix with input data from which outputs are predicted\n",
            " |          fY (hdf5): hdf5 filename or Numpy matrix to store output data into, if 'None' then Numpy matrix\n",
            " |              is generated automatically.\n",
            " |          istart (int, optional): index of first data sample to use from `fX`, `istart` < N. If not given,\n",
            " |              all data from `fX` is used. Sample with index `istart` is used for training, indexing is 0-based.\n",
            " |          icount (int, optional): number of data samples to use from `fX`, starting from `istart`, automatically\n",
            " |              adjusted to `istart` + `icount` <= N. If not given, all data starting from `start` is used.\n",
            " |              The last sample used for training is `istart`+`icount`-1, so you can index data as:\n",
            " |              istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...\n",
            " |\n",
            " |  predict_async(self, fX, fY, istart=0, icount=inf)\n",
            " |      Version of `predict()` with asyncronous I/O. See `predict()` for reference.\n",
            " |\n",
            " |      Spawns new processes using Python's `multiprocessing` module, and requires more memory than non-async version.\n",
            " |\n",
            " |  project(self, fX, fH=None, istart=0, icount=inf)\n",
            " |      Iteratively project input data from HDF5 into HPELM hidden layer, and save in another HDF5.\n",
            " |\n",
            " |      Args:\n",
            " |          fX (hdf5): hdf5 filename or Numpy matrix with input data to project\n",
            " |          fH (hdf5): hdf5 filename or Numpy matrix to store projected inputs, if 'None' then Numpy matrix\n",
            " |              is generated automatically.\n",
            " |          istart (int, optional): index of first data sample to use from `fX`, `istart` < N. If not given,\n",
            " |              all data from `fX` is used. Sample with index `istart` is used for training, indexing is 0-based.\n",
            " |          icount (int, optional): number of data samples to use from `fX`, starting from `istart`, automatically\n",
            " |              adjusted to `istart` + `icount` <= N. If not given, all data starting from `start` is used.\n",
            " |              The last sample used for training is `istart`+`icount`-1, so you can index data as:\n",
            " |              istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...\n",
            " |\n",
            " |  solve_corr(self, fHH, fHT)\n",
            " |      Solves an ELM model with the given (covariance) fHH and (correlation) fHT HDF5 files.\n",
            " |\n",
            " |      Args:\n",
            " |          fHH (hdf5): an hdf5 file with intermediate solution data\n",
            " |          fHT (hdf5): an hdf5 file with intermediate solution data\n",
            " |\n",
            " |  train(self, fX, fT, *args, **kwargs)\n",
            " |      Universal training interface for HP-ELM model.\n",
            " |\n",
            " |      Always trains a basic ELM model without model structure selection.\n",
            " |      L2-regularization is available as `norm` parameter at HPELM initialization.\n",
            " |      Number of neurons selection with validation set for trained HPELM is available in `train_hpv()` method.\n",
            " |\n",
            " |      Args:\n",
            " |          fX (hdf5): input data on disk, size (N * `inputs`)\n",
            " |          fT (hdf5): outputs data on disk, size (N * `outputs`)\n",
            " |          'c'/'wc'/'ml' (string, choose one): train HPELM for classification ('c'), classification with weighted\n",
            " |              classes ('wc') or multi-label classification ('ml') with several correct classes per data sample.\n",
            " |              In classification, number of `outputs` is the number of classes; correct class(es) for each sample\n",
            " |              has value 1 and incorrect classes have 0.\n",
            " |\n",
            " |      Keyword Args:\n",
            " |          istart (int, optional): index of first data sample to use from `fX`, `istart` < N. If not given,\n",
            " |              all data from `fX` is used. Sample with index `istart` is used for training, indexing is 0-based.\n",
            " |          icount (int, optional): number of data samples to use from `fX`, starting from `istart`, automatically\n",
            " |              adjusted to `istart` + `icount` <= N. If not given, all data starting from `start` is used.\n",
            " |              The last sample used for training is `istart`+`icount`-1, so you can index data as:\n",
            " |              istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...\n",
            " |          batch (int, optional): batch size for ELM, overwrites batch size from the initialization\n",
            " |\n",
            " |  train_async(self, fX, fT, *args, **kwargs)\n",
            " |      Training HPELM with asyncronous I/O, good for network drives, etc. See `train()` for reference.\n",
            " |\n",
            " |      Spawns new processes using Python's `multiprocessing` module.\n",
            " |\n",
            " |  validation_corr(self, fHH, fHT, fXv, fTv, steps=10)\n",
            " |      Quick batch error evaluation with different numbers of neurons on a validation set.\n",
            " |\n",
            " |      Only feasible implementation of model structure selection with HP-ELM. This method makes a single pass\n",
            " |      over the validation data, computing errors for all numbers of neurons at once. It requires HDF5 files with\n",
            " |      matrices HH and HT: `fHH` and `fHT`, obtained from `add_data(..., fHH, fHT)` method.\n",
            " |\n",
            " |      The method writes the best solution to the HPELM model.\n",
            " |\n",
            " |      Args:\n",
            " |          fHH (string): name of HDF5 file with HH matrix\n",
            " |          fHT (string): name of HDF5 file with HT matrix\n",
            " |          fXv (string): name of HDF5 file with validation dataset inputs\n",
            " |          fTv (string): name of HDF5 file with validation dataset outputs\n",
            " |          steps (int or vector): amount of different numbers of neurons to test, choosen uniformly on a logarithmic\n",
            " |              scale from 3 to number of neurons in HPELM. Can be given exactly as a vector.\n",
            " |\n",
            " |      Returns:\n",
            " |          Ls (vector): numbers of neurons used by `validation_corr()` method\n",
            " |          errs (vector): corresponding errors for number of neurons in `Ls`, with classification error if model\n",
            " |              is run for classification\n",
            " |          confs (list of matrix): list of confusion matrices corresponding to elements in Ls (empty for regression)\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from hpelm.elm.ELM:\n",
            " |\n",
            " |  __del__(self)\n",
            " |\n",
            " |  __init__(self, inputs, outputs, classification='', w=None, batch=1000, accelerator=None, precision='double', norm=None, tprint=5)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |\n",
            " |  __str__(self)\n",
            " |      Return str(self).\n",
            " |\n",
            " |  add_neurons(self, number, func, W=None, B=None)\n",
            " |      Adds neurons to ELM model. ELM is created empty, and needs some neurons to work.\n",
            " |\n",
            " |      Add neurons to an empty ELM model, or add more neurons to a model that already has some.\n",
            " |\n",
            " |      Random weights `W` and biases `B` are generated automatically if not provided explicitly.\n",
            " |      Maximum number of neurons is limited by the available RAM and computational power, a sensible limit\n",
            " |      would be 1000 neurons for an average size dataset and 15000 for the largest datasets. ELM becomes slower after\n",
            " |      3000 neurons because computational complexity is proportional to a qube of number of neurons.\n",
            " |\n",
            " |      This method checks and prepares neurons, they are actually stored in `solver` object.\n",
            " |\n",
            " |      Args:\n",
            " |          number (int): number of neurons to add\n",
            " |          func (string): type of neurons: \"lin\" for linear, \"sigm\" or \"tanh\" for non-linear,\n",
            " |              \"rbf_l1\", \"rbf_l2\" or \"rbf_linf\" for radial basis function neurons.\n",
            " |          W (matrix, optional): random projection matrix size (`inputs` * `number`). For 'rbf_' neurons,\n",
            " |              W stores centroids of radial basis functions in transposed form.\n",
            " |          B (vector, optional): bias vector of size (`number` * 1), a 1-dimensional Numpy.ndarray object.\n",
            " |              For 'rbf_' neurons, B gives widths of radial basis functions.\n",
            " |\n",
            " |  confusion(self, T, Y)\n",
            " |      Computes confusion matrix for classification.\n",
            " |\n",
            " |      Confusion matrix :math:`C` such that element :math:`C_{i,j}` equals to the number of observations known\n",
            " |      to be class :math:`i` but predicted to be class :math:`j`.\n",
            " |\n",
            " |      Args:\n",
            " |          T (matrix): true outputs or classes, size (N * `outputs`)\n",
            " |          Y (matrix): predicted outputs by ELM model, size (N * `outputs`)\n",
            " |\n",
            " |      Returns:\n",
            " |          conf (matrix): confusion matrix, size (`outputs` * `outputs`)\n",
            " |\n",
            " |  load(self, fname)\n",
            " |      Load ELM model data from a file.\n",
            " |\n",
            " |      Load requires an ``ELM`` object, and it uses solver type, precision and batch size from that ELM object.\n",
            " |\n",
            " |      Args:\n",
            " |          fname (string): filename to load model from.\n",
            " |\n",
            " |  save(self, fname)\n",
            " |      Save ELM model with current parameters.\n",
            " |\n",
            " |      Model does not save a particular solver, precision batch size. They are obtained from\n",
            " |      a new ELM when loading the model (so one can switch to another solver, for instance).\n",
            " |\n",
            " |      Also ranking and max number of neurons are not saved, because they\n",
            " |      are runtime training info irrelevant after the training completes.\n",
            " |\n",
            " |      Args:\n",
            " |          fname (string): filename to save model into.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from hpelm.elm.ELM:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(HPELM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fadc4ca-1a57-4b76-b29a-f57d6285869e",
      "metadata": {
        "id": "7fadc4ca-1a57-4b76-b29a-f57d6285869e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a710b74-ec4e-4971-b43c-a67f442a6451",
      "metadata": {
        "id": "0a710b74-ec4e-4971-b43c-a67f442a6451"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "f81d3ea6-7eae-407c-86e3-fbaaceb00fb5",
      "metadata": {
        "id": "f81d3ea6-7eae-407c-86e3-fbaaceb00fb5",
        "outputId": "591481c1-5fdb-49be-ff5d-abcedf6dc4ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8732\n",
            "Precision: 0.8779\n",
            "Recall: 0.8671\n",
            "F1-Score: 0.8724\n",
            "ROC-AUC: 0.8953\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87      1700\n",
            "           1       0.88      0.87      0.87      1700\n",
            "\n",
            "    accuracy                           0.87      3400\n",
            "   macro avg       0.87      0.87      0.87      3400\n",
            "weighted avg       0.87      0.87      0.87      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, classification_report, precision_recall_curve)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, max_iter=500, solver='saga', C=0.1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_processed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_processed)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "e731a829-7f87-433f-a823-80ebbedad735",
      "metadata": {
        "id": "e731a829-7f87-433f-a823-80ebbedad735",
        "outputId": "8725fefc-6328-4fdd-cf5a-2a2455fe4940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8732\n",
            "Precision: 0.8779\n",
            "Recall: 0.8671\n",
            "F1-Score: 0.8724\n",
            "ROC-AUC: 0.8953\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87      1700\n",
            "           1       0.88      0.87      0.87      1700\n",
            "\n",
            "    accuracy                           0.87      3400\n",
            "   macro avg       0.87      0.87      0.87      3400\n",
            "weighted avg       0.87      0.87      0.87      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, classification_report, precision_recall_curve)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, max_iter=500, solver='lbfgs', C=0.1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_processed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_processed)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff1b3ad-f20f-4601-ab61-38e7b7a58189",
      "metadata": {
        "id": "1ff1b3ad-f20f-4601-ab61-38e7b7a58189"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "9970db3a-f9e8-4c39-8e89-db6017343495",
      "metadata": {
        "id": "9970db3a-f9e8-4c39-8e89-db6017343495",
        "outputId": "f9b33bcd-93a4-47f6-de95-36c67d59607e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'max_iter': 100, 'solver': 'saga'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "param_grid = {\n",
        "    'solver': ['lbfgs', 'saga'],\n",
        "    'C': [0.1, 1, 10],\n",
        "    'max_iter': [100, 200, 500]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=LogisticRegression(),\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',  # Use appropriate scoring metric\n",
        "    cv=5  # Number of folds in cross-validation\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_processed, y_train)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ead63b-3b8b-4a91-b751-2df95ea1b7e4",
      "metadata": {
        "id": "00ead63b-3b8b-4a91-b751-2df95ea1b7e4"
      },
      "outputs": [],
      "source": [
        "# help(GridSearchCV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "0fa4e7e3-bcbf-4813-a01c-f3501d51d7f4",
      "metadata": {
        "id": "0fa4e7e3-bcbf-4813-a01c-f3501d51d7f4",
        "outputId": "4b43eb67-39ca-4cc9-831b-47c7ccf845d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8732\n",
            "Precision: 0.8779\n",
            "Recall: 0.8671\n",
            "F1-Score: 0.8724\n",
            "ROC-AUC: 0.8953\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87      1700\n",
            "           1       0.88      0.87      0.87      1700\n",
            "\n",
            "    accuracy                           0.87      3400\n",
            "   macro avg       0.87      0.87      0.87      3400\n",
            "weighted avg       0.87      0.87      0.87      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Initialize Logistic Regression model with the tuned parameters\n",
        "model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=100,\n",
        "    solver='saga',\n",
        "    C=0.1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_processed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_processed)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "aa9b3efd-2ebc-44bc-91ac-8880ee0f9968",
      "metadata": {
        "id": "aa9b3efd-2ebc-44bc-91ac-8880ee0f9968",
        "outputId": "89bc300b-92b2-4a62-af9f-f87f245ccd01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics\n",
            "=====================================\n",
            "Accuracy: 0.8724\n",
            "Precision: 0.8777\n",
            "Recall: 0.8653\n",
            "F1-Score: 0.8714\n",
            "ROC-AUC: 0.8724\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Regularized Logistic Regression model\n",
        "# L1 Regularization (lasso): `penalty='l1'`\n",
        "# L2 Regularization (ridge): `penalty='l2'`\n",
        "model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    solver='saga',\n",
        "    C=0.1,  # Regularization strength\n",
        "    penalty='l1'  # Use L1 or L2\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_processed, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_processed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"Evaluation Metrics\")\n",
        "print(\"=====================================\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b9c58093-1423-4acb-93a7-148b55448fe7",
      "metadata": {
        "id": "b9c58093-1423-4acb-93a7-148b55448fe7",
        "outputId": "63e0bdba-476a-465f-8454-f2d13a7713eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Best Cross-Validation Accuracy: 0.8866911764705883\n",
            "Evaluation Metrics for Best Model\n",
            "=====================================\n",
            "Accuracy: 0.8732\n",
            "Precision: 0.8761\n",
            "Recall: 0.8694\n",
            "F1-Score: 0.8727\n",
            "ROC-AUC: 0.8732\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Define the parameter grid for regularization strength\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Values to test for regularization strength\n",
        "    'solver': ['saga'],  # Saga supports both L1 and L2 regularization\n",
        "    'penalty': ['l1', 'l2']  # Test both L1 (Lasso) and L2 (Ridge)\n",
        "}\n",
        "\n",
        "# Grid search for best regularization strength\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(random_state=42, max_iter=500),\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',  # You can switch to precision, recall, or other metrics\n",
        "    cv=5  # 5-fold cross-validation\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_processed, y_train)\n",
        "\n",
        "# Print the best parameters and corresponding score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Use the best model for evaluation on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_processed)\n",
        "\n",
        "# Evaluate the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"Evaluation Metrics for Best Model\")\n",
        "print(\"=====================================\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "c0f353c6-f0da-4b56-b75f-5c2a1f19985a",
      "metadata": {
        "id": "c0f353c6-f0da-4b56-b75f-5c2a1f19985a",
        "outputId": "17ea622c-73db-4295-be47-452e1282adfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.01, 'l1_ratio': 0.5, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "Best Cross-Validation Accuracy: 0.8857352941176471\n",
            "\n",
            "Evaluation Metrics for ElasticNet Regularization\n",
            "====================================================\n",
            "Accuracy: 0.8712\n",
            "Precision: 0.8760\n",
            "Recall: 0.8647\n",
            "F1-Score: 0.8703\n",
            "ROC-AUC: 0.8712\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Define the parameter grid for ElasticNet regularization\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],  # Regularization strengths\n",
        "    'l1_ratio': [0.2, 0.5, 0.8],  # Mix of L1 and L2 penalties\n",
        "    'solver': ['saga'],  # Saga supports ElasticNet\n",
        "    'penalty': ['elasticnet']  # Use ElasticNet penalty\n",
        "}\n",
        "\n",
        "# Grid search to find the best ElasticNet parameters\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(random_state=42, max_iter=500),\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',  # You can also use precision, recall, etc.\n",
        "    cv=5  # 5-fold cross-validation\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_processed, y_train)\n",
        "\n",
        "# Print the best parameters and corresponding score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Use the best model for evaluation on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_processed)\n",
        "\n",
        "# Evaluate the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nEvaluation Metrics for ElasticNet Regularization\")\n",
        "print(\"====================================================\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9920dd7-e1e1-4507-b45b-00d838e772f0",
      "metadata": {
        "id": "a9920dd7-e1e1-4507-b45b-00d838e772f0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c913bcca-2e27-40dd-ad44-0dee24a36975",
      "metadata": {
        "id": "c913bcca-2e27-40dd-ad44-0dee24a36975"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc3862eb-82b6-40a8-94f5-4737584cac1a",
      "metadata": {
        "id": "fc3862eb-82b6-40a8-94f5-4737584cac1a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a604a851-3297-48db-a9b9-6c3e6cb15ed6",
      "metadata": {
        "id": "a604a851-3297-48db-a9b9-6c3e6cb15ed6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2015041-410e-44c7-8f12-1d6dd9f7f2c6",
      "metadata": {
        "id": "d2015041-410e-44c7-8f12-1d6dd9f7f2c6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9dc64cb-943e-4c1b-ba8a-3ffddf25ef6c",
      "metadata": {
        "id": "e9dc64cb-943e-4c1b-ba8a-3ffddf25ef6c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e89395c3-f6f5-4f5e-9593-987bd2b2ddc1",
      "metadata": {
        "id": "e89395c3-f6f5-4f5e-9593-987bd2b2ddc1"
      },
      "outputs": [],
      "source": [
        "# Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "5bc3002c-9368-4220-a1e1-26d4bf9438be",
      "metadata": {
        "id": "5bc3002c-9368-4220-a1e1-26d4bf9438be",
        "outputId": "189f2303-befb-42e8-f692-81db5cc38333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.9021\n",
            "Precision: 0.8926\n",
            "Recall: 0.9141\n",
            "F1-Score: 0.9032\n",
            "ROC-AUC: 0.8953\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.89      0.90      1700\n",
            "           1       0.89      0.91      0.90      1700\n",
            "\n",
            "    accuracy                           0.90      3400\n",
            "   macro avg       0.90      0.90      0.90      3400\n",
            "weighted avg       0.90      0.90      0.90      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize Random Forest model\n",
        "model = RandomForestClassifier(n_estimators=2500, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_processed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_processed)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "747c557c-e2b6-4bca-a5b7-a923011c3f54",
      "metadata": {
        "id": "747c557c-e2b6-4bca-a5b7-a923011c3f54"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2535d06b-b704-497a-97a1-e1a2e721e6b4",
      "metadata": {
        "id": "2535d06b-b704-497a-97a1-e1a2e721e6b4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20d8c89d-add6-4066-b854-c7f6999b3492",
      "metadata": {
        "id": "20d8c89d-add6-4066-b854-c7f6999b3492"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine(SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "9620f191-1513-4b19-b975-6d8a05be4e3c",
      "metadata": {
        "id": "9620f191-1513-4b19-b975-6d8a05be4e3c",
        "outputId": "919205b5-7141-4ce7-a570-824d540b0df1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8803\n",
            "Precision: 0.8761\n",
            "Recall: 0.8859\n",
            "F1-Score: 0.8810\n",
            "ROC-AUC: 0.8953\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.88      1700\n",
            "           1       0.88      0.89      0.88      1700\n",
            "\n",
            "    accuracy                           0.88      3400\n",
            "   macro avg       0.88      0.88      0.88      3400\n",
            "weighted avg       0.88      0.88      0.88      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model\n",
        "model = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_processed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_processed)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "374e97b4-2200-4c73-ba18-900c7859f5bf",
      "metadata": {
        "id": "374e97b4-2200-4c73-ba18-900c7859f5bf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6065a4c-f46a-46c7-9eff-5e0aa3ff6a32",
      "metadata": {
        "id": "a6065a4c-f46a-46c7-9eff-5e0aa3ff6a32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312b82e9-dd3b-4d2a-b267-2333176f2126",
      "metadata": {
        "id": "312b82e9-dd3b-4d2a-b267-2333176f2126"
      },
      "outputs": [],
      "source": [
        "# K-Nearest Neighbourh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "f5439170-6fef-4e0a-9b4e-ea8e62e45a04",
      "metadata": {
        "id": "f5439170-6fef-4e0a-9b4e-ea8e62e45a04",
        "outputId": "e02e52c2-9b99-4314-ede6-69ed2094aabc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics\n",
            "==================================================\n",
            "Accuracy: 0.8679\n",
            "Precision: 0.8564\n",
            "Recall: 0.8841\n",
            "F1-Score: 0.8700\n",
            "ROC-AUC: 0.8953\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.87      1700\n",
            "           1       0.86      0.88      0.87      1700\n",
            "\n",
            "    accuracy                           0.87      3400\n",
            "   macro avg       0.87      0.87      0.87      3400\n",
            "weighted avg       0.87      0.87      0.87      3400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize KNN model\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_processed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_processed)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nEvaluation Metrics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3fd1412-5de3-4504-9b96-d6080b7c86af",
      "metadata": {
        "id": "d3fd1412-5de3-4504-9b96-d6080b7c86af"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5246d9-51ee-411c-a035-56d4a5af1232",
      "metadata": {
        "id": "de5246d9-51ee-411c-a035-56d4a5af1232"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9aa1caac-0ce3-424b-b2d3-000caed7e360",
      "metadata": {
        "id": "9aa1caac-0ce3-424b-b2d3-000caed7e360"
      },
      "source": [
        "# Voting Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "f79e4965-57e6-4746-af78-48f1f0bec0d7",
      "metadata": {
        "id": "f79e4965-57e6-4746-af78-48f1f0bec0d7",
        "outputId": "42909287-80dd-426c-c8b0-1d19c20b1024",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Ensemble Metrics\n",
            "=================================\n",
            "Accuracy: 0.8888\n",
            "Precision: 0.8865\n",
            "Recall: 0.8918\n",
            "F1-Score: 0.8891\n",
            "ROC-AUC: 0.8888\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Initialize individual models\n",
        "log_reg = LogisticRegression(random_state=42, C=0.01, penalty='l2', solver='saga', max_iter=500)  # Best-tuned LR model\n",
        "rf_clf = RandomForestClassifier(random_state=42, n_estimators=100)  # Random Forest\n",
        "svm_clf = SVC(probability=True, random_state=42)  # Support Vector Machine with probability for soft voting\n",
        "\n",
        "# Create the VotingClassifier (use 'soft' or 'hard' voting)\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_reg), ('rf', rf_clf), ('svm', svm_clf)],\n",
        "    voting='soft'  # For soft voting based on probabilities\n",
        ")\n",
        "\n",
        "# Train the ensemble model\n",
        "voting_clf.fit(X_train_processed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = voting_clf.predict(X_test_processed)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"Voting Ensemble Metrics\")\n",
        "print(\"=================================\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7841555-1e8e-4383-8e98-9d15faad3b06",
      "metadata": {
        "id": "c7841555-1e8e-4383-8e98-9d15faad3b06"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e89dcdda-4b8a-4bc3-ac72-c508ab3f99c0",
      "metadata": {
        "id": "e89dcdda-4b8a-4bc3-ac72-c508ab3f99c0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0a7541f8-6a7f-4562-93c9-e347ec867d6b",
      "metadata": {
        "id": "0a7541f8-6a7f-4562-93c9-e347ec867d6b"
      },
      "source": [
        "# Stacking Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "e23f6c8b-ac15-4cf7-beec-5b1d3039d3f7",
      "metadata": {
        "id": "e23f6c8b-ac15-4cf7-beec-5b1d3039d3f7",
        "outputId": "67190330-7942-4d52-cd03-846a4dd91e11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Ensemble Metrics\n",
            "=================================\n",
            "Accuracy: 0.8974\n",
            "Precision: 0.8907\n",
            "Recall: 0.9059\n",
            "F1-Score: 0.8982\n",
            "ROC-AUC: 0.8974\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(random_state=42, n_estimators=100)),  # Random Forest\n",
        "    ('svm', SVC(probability=True, random_state=42))  # Support Vector Machine with probability output\n",
        "]\n",
        "\n",
        "# Define the meta-model (Logistic Regression in this case)\n",
        "meta_model = LogisticRegression(random_state=42, solver='saga', C=0.1, penalty='l2')\n",
        "\n",
        "# Create the StackingClassifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5  # Cross-validation during training of the meta-model\n",
        ")\n",
        "\n",
        "# Train the Stacking Ensemble\n",
        "stacking_clf.fit(X_train_processed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stacking_clf.predict(X_test_processed)\n",
        "\n",
        "# Evaluate the Stacking Ensemble\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"Stacking Ensemble Metrics\")\n",
        "print(\"=================================\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff81a593-a81a-47b6-8c12-809b708eb0d1",
      "metadata": {
        "id": "ff81a593-a81a-47b6-8c12-809b708eb0d1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "02719a1a-e10c-47ca-ab78-da8e41cb7fbe",
      "metadata": {
        "id": "02719a1a-e10c-47ca-ab78-da8e41cb7fbe"
      },
      "source": [
        "## Tuning the Base Models and Meta-Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "625e9b65-509c-44d8-a4a0-249267ddef12",
      "metadata": {
        "id": "625e9b65-509c-44d8-a4a0-249267ddef12"
      },
      "source": [
        "### Step 1: Tune the Base Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "399a63f3-0503-4d5a-9af1-8578fc28c794",
      "metadata": {
        "id": "399a63f3-0503-4d5a-9af1-8578fc28c794",
        "outputId": "c2aa9636-4e88-4184-dcaf-c3cf9506a2b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest Parameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 200}\n",
            "Best SVM Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Tuning Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees\n",
        "    'max_depth': [10, 20, None],     # Maximum depth of trees\n",
        "    'min_samples_split': [2, 5, 10]  # Minimum samples to split a node\n",
        "}\n",
        "\n",
        "rf_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid=rf_param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5\n",
        ")\n",
        "rf_search.fit(X_train_processed, y_train)\n",
        "best_rf = rf_search.best_estimator_\n",
        "print(\"Best Random Forest Parameters:\", rf_search.best_params_)\n",
        "\n",
        "# Tuning SVM\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10],           # Regularization parameter\n",
        "    'kernel': ['linear', 'rbf'],  # Kernel type\n",
        "    'gamma': ['scale', 'auto']    # Kernel coefficient\n",
        "}\n",
        "\n",
        "svm_search = GridSearchCV(\n",
        "    SVC(probability=True, random_state=42),\n",
        "    param_grid=svm_param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5\n",
        ")\n",
        "svm_search.fit(X_train_processed, y_train)\n",
        "best_svm = svm_search.best_estimator_\n",
        "print(\"Best SVM Parameters:\", svm_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2664cb8b-9fcf-41f4-91e0-fd3264fbb4ac",
      "metadata": {
        "id": "2664cb8b-9fcf-41f4-91e0-fd3264fbb4ac"
      },
      "source": [
        "### Step 2: Tune the Meta-Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "31a33dab-30be-4df6-a379-2c2407b60c33",
      "metadata": {
        "id": "31a33dab-30be-4df6-a379-2c2407b60c33",
        "outputId": "029ec2b1-2cb2-4252-d80d-a1c672f3f0cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression Parameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Tuning Logistic Regression (meta-model)\n",
        "lr_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
        "    'solver': ['saga'],       # Solver for ElasticNet\n",
        "    'penalty': ['l1', 'l2']   # Regularization type\n",
        "}\n",
        "\n",
        "lr_search = GridSearchCV(\n",
        "    LogisticRegression(random_state=42, max_iter=500),\n",
        "    param_grid=lr_param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5\n",
        ")\n",
        "lr_search.fit(X_train_processed, y_train)\n",
        "best_meta_model = lr_search.best_estimator_\n",
        "print(\"Best Logistic Regression Parameters:\", lr_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66228d97-567e-486f-9641-66bd6910483c",
      "metadata": {
        "id": "66228d97-567e-486f-9641-66bd6910483c"
      },
      "source": [
        "### Step 3: Integrate Tuned Models into Stacking Ensemble\n",
        "Finally, use the tuned base models (best_rf, best_svm) and the meta-model (best_meta_model) in the stacking classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "3955e299-15ed-4d80-8c79-4fb1d89a4cb7",
      "metadata": {
        "id": "3955e299-15ed-4d80-8c79-4fb1d89a4cb7",
        "outputId": "8629144b-d558-4f83-f035-9489d82a79bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimized Stacking Ensemble Metrics\n",
            "======================================\n",
            "Accuracy: 0.9000\n",
            "Precision: 0.8981\n",
            "Recall: 0.9024\n",
            "F1-Score: 0.9002\n",
            "ROC-AUC: 0.9000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Define tuned models\n",
        "base_models = [\n",
        "    ('rf', best_rf),\n",
        "    ('svm', best_svm)\n",
        "]\n",
        "\n",
        "# Stacking classifier with tuned models\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=best_meta_model,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train the optimized Stacking Ensemble\n",
        "stacking_clf.fit(X_train_processed, y_train)\n",
        "\n",
        "# Evaluate the ensemble\n",
        "y_pred = stacking_clf.predict(X_test_processed)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nOptimized Stacking Ensemble Metrics\")\n",
        "print(\"======================================\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f7a7eaa-c0a2-4243-aac2-3434f32b7cdc",
      "metadata": {
        "id": "2f7a7eaa-c0a2-4243-aac2-3434f32b7cdc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97058ee5-0442-4cf5-8783-4547ed66a44f",
      "metadata": {
        "id": "97058ee5-0442-4cf5-8783-4547ed66a44f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6f797821-63bb-40b0-b705-09c198c3b180",
      "metadata": {
        "id": "6f797821-63bb-40b0-b705-09c198c3b180"
      },
      "source": [
        "# Bagging Ensemble Model\n",
        "Bagging, short for Bootstrap Aggregating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "05cd957b-a65b-464e-bb9b-a49fc386250a",
      "metadata": {
        "id": "05cd957b-a65b-464e-bb9b-a49fc386250a",
        "outputId": "646e11f5-5f95-4af7-879c-af1e383d12a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bagging Ensemble Metrics\n",
            "=======================================\n",
            "Accuracy: 0.8976\n",
            "Precision: 0.8890\n",
            "Recall: 0.9088\n",
            "F1-Score: 0.8988\n",
            "ROC-AUC: 0.8976\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Initialize base model (Decision Tree in this case)\n",
        "base_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create BaggingClassifier\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=base_model,  # Correct keyword for base model\n",
        "    n_estimators=50,       # Number of base models\n",
        "    max_samples=0.8,       # Fraction of samples used for training each base model\n",
        "    max_features=0.8,      # Fraction of features used for training each base model\n",
        "    bootstrap=True,        # Enable bootstrapping of samples\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging_clf.fit(X_train_processed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test_processed)\n",
        "\n",
        "# Evaluate the Bagging model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nBagging Ensemble Metrics\")\n",
        "print(\"=======================================\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b86e8f79-e819-4b8b-bc53-a35a8f17bcf2",
      "metadata": {
        "id": "b86e8f79-e819-4b8b-bc53-a35a8f17bcf2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f676a889-d27d-4b67-888d-20590758d8c3",
      "metadata": {
        "id": "f676a889-d27d-4b67-888d-20590758d8c3"
      },
      "source": [
        "## hyperparameter Tuning for Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "4324f710-67fc-47b8-83cc-e01cdd1d3817",
      "metadata": {
        "id": "4324f710-67fc-47b8-83cc-e01cdd1d3817",
        "outputId": "76217d37-119f-4524-cbfb-1156c9076972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'estimator__max_depth': None, 'estimator__min_samples_split': 10, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 100}\n",
            "Best Cross-Validation Accuracy: 0.9098529411764705\n",
            "\n",
            "Optimized Bagging Ensemble Metrics\n",
            "=======================================\n",
            "Accuracy: 0.9074\n",
            "Precision: 0.8924\n",
            "Recall: 0.9265\n",
            "F1-Score: 0.9091\n",
            "ROC-AUC: 0.9074\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train_processed)\n",
        "X_test_processed = scaler.transform(X_test_processed)\n",
        "\n",
        "# Define the Bagging model with a Decision Tree as the base estimator\n",
        "base_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Parameter grid for Bagging\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],  # Number of estimators\n",
        "    'max_samples': [0.5, 0.8, 1.0],  # Fraction of training samples for each base estimator\n",
        "    'max_features': [0.5, 0.8, 1.0],  # Fraction of features for each base estimator\n",
        "    'estimator__max_depth': [3, 5, None],  # Max depth of the base Decision Tree\n",
        "    'estimator__min_samples_split': [2, 5, 10]  # Min samples to split a node\n",
        "}\n",
        "\n",
        "# GridSearchCV for BaggingClassifier\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=base_model,\n",
        "    bootstrap=True,  # Bootstrap samples\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=bagging_clf,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',  # Evaluate using accuracy\n",
        "    cv=5  # 5-fold cross-validation\n",
        ")\n",
        "\n",
        "# Fit the grid search on the training data\n",
        "grid_search.fit(X_train_processed, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_bagging_clf = grid_search.best_estimator_\n",
        "y_pred = best_bagging_clf.predict(X_test_processed)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nOptimized Bagging Ensemble Metrics\")\n",
        "print(\"=======================================\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1596b66-8c12-4615-9ad1-19365d0911f3",
      "metadata": {
        "id": "b1596b66-8c12-4615-9ad1-19365d0911f3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c7a5be1-79e7-403d-98fa-aebc9afdac92",
      "metadata": {
        "id": "5c7a5be1-79e7-403d-98fa-aebc9afdac92"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d32ce40-8b2e-4d66-8683-efc1c6f94f8c",
      "metadata": {
        "id": "8d32ce40-8b2e-4d66-8683-efc1c6f94f8c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e1a4faa-a326-4330-a4f0-629430b19e9e",
      "metadata": {
        "id": "5e1a4faa-a326-4330-a4f0-629430b19e9e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}